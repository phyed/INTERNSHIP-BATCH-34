{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "508851bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bs4 in c:\\users\\phyed\\appdata\\roaming\\python\\python39\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.27.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ef6c3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cab1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7713db2",
   "metadata": {},
   "source": [
    "# ASSIGNMENT SOLVE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e8edaf",
   "metadata": {},
   "source": [
    "#  Q4 .Write s python program to display list of respected former presidents of India(SOLVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1ba8eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "['\\nShri Ram Nath Kovind (birth - 1945)\\nTerm of Office: 25 July, 2017 to 25 July, 2022 \\nhttps://ramnathkovind.nic.in\\n', '\\nShri Pranab Mukherjee (1935-2020)\\nTerm of Office: 25 July, 2012 to 25 July, 2017 \\nhttp://pranabmukherjee.nic.in\\n', '\\nSmt Pratibha Devisingh Patil (birth - 1934)\\nTerm of Office: 25 July, 2007 to 25 July, 2012 \\nhttp://pratibhapatil.nic.in\\n', '\\nDR. A.P.J. Abdul Kalam (1931-2015)\\nTerm of Office: 25 July, 2002 to 25 July, 2007 \\nhttp://abdulkalam.nic.in\\n', '\\nShri K. R. Narayanan (1920 - 2005)\\nTerm of Office: 25 July, 1997 to 25 July, 2002 \\n', '\\nDr Shankar Dayal Sharma (1918-1999)\\nTerm of Office: 25 July, 1992 to 25 July, 1997 \\n', '\\nShri R Venkataraman (1910-2009)\\nTerm of Office: 25 July, 1987 to 25 July, 1992 \\n', '\\nGiani Zail Singh (1916-1994)\\nTerm of Office: 25 July, 1982 to 25 July, 1987 \\n', '\\nShri Neelam Sanjiva Reddy (1913-1996)\\nTerm of Office: 25 July, 1977 to 25 July, 1982 \\n', '\\nDr. Fakhruddin Ali Ahmed (1905-1977)\\nTerm of Office: 24 August, 1974 to 11 February, 1977\\n', '\\nShri Varahagiri Venkata Giri (1894-1980)\\nTerm of Office: 3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974\\n', '\\nDr. Zakir Husain (1897-1969)\\nTerm of Office: 13 May, 1967 to 3 May, 1969\\n', '\\nDr. Sarvepalli Radhakrishnan (1888-1975)\\nTerm of Office: 13 May, 1962 to 13 May, 1967\\n', '\\nDr. Rajendra Prasad (1884-1963) \\nTerm of Office: 26 January, 1950 to 13 May, 1962\\n']\n"
     ]
    }
   ],
   "source": [
    "page_president=requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "print(page_president)\n",
    "soup_president=BeautifulSoup(page_president.content)\n",
    "soup_president\n",
    "titles_president=[]\n",
    "for i in soup_president.find_all('div',class_='presidentListing'):\n",
    "    titles_president.append(i.text)\n",
    "print(titles_president)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3e00ad84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0   \\nShri Ram Nath Kovind (birth - 1945)\\nTerm of...\n",
      "1   \\nShri Pranab Mukherjee (1935-2020)\\nTerm of O...\n",
      "2   \\nSmt Pratibha Devisingh Patil (birth - 1934)\\...\n",
      "3   \\nDR. A.P.J. Abdul Kalam (1931-2015)\\nTerm of ...\n",
      "4   \\nShri K. R. Narayanan (1920 - 2005)\\nTerm of ...\n",
      "5   \\nDr Shankar Dayal Sharma (1918-1999)\\nTerm of...\n",
      "6   \\nShri R Venkataraman (1910-2009)\\nTerm of Off...\n",
      "7   \\nGiani Zail Singh (1916-1994)\\nTerm of Office...\n",
      "8   \\nShri Neelam Sanjiva Reddy (1913-1996)\\nTerm ...\n",
      "9   \\nDr. Fakhruddin Ali Ahmed (1905-1977)\\nTerm o...\n",
      "10  \\nShri Varahagiri Venkata Giri (1894-1980)\\nTe...\n",
      "11  \\nDr. Zakir Husain (1897-1969)\\nTerm of Office...\n",
      "12  \\nDr. Sarvepalli Radhakrishnan (1888-1975)\\nTe...\n",
      "13  \\nDr. Rajendra Prasad (1884-1963) \\nTerm of Of...\n"
     ]
    }
   ],
   "source": [
    "df=pd.DataFrame(titles_president) # converting into data frame \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d863505",
   "metadata": {},
   "source": [
    "# Q5. Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating(SOLVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c1b0e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                   0\n",
      "0        New Zealand\n",
      "1            England\n",
      "2          Australia\n",
      "3              India\n",
      "4           Pakistan\n",
      "5       South Africa\n",
      "6         Bangladesh\n",
      "7          Sri Lanka\n",
      "8        Afghanistan\n",
      "9        West Indies\n",
      "10           Ireland\n",
      "11          Scotland\n",
      "12          Zimbabwe\n",
      "13           Namibia\n",
      "14       Netherlands\n",
      "15              Oman\n",
      "16               UAE\n",
      "17     United States\n",
      "18             Nepal\n",
      "19  Papua New Guinea\n",
      "        0\n",
      "0      30\n",
      "1   3,400\n",
      "2      32\n",
      "3   3,572\n",
      "4      35\n",
      "5   3,866\n",
      "6      22\n",
      "7   2,354\n",
      "8      24\n",
      "9   2,392\n",
      "10     30\n",
      "11  2,753\n",
      "12     30\n",
      "13  2,677\n",
      "14     19\n",
      "15  1,380\n",
      "16     41\n",
      "17  2,902\n",
      "18     23\n",
      "19  1,214\n",
      "20     27\n",
      "21  1,254\n",
      "22     26\n",
      "23  1,098\n",
      "24     23\n",
      "25    808\n",
      "26     21\n",
      "27    673\n",
      "28     30\n",
      "29    919\n",
      "30     25\n",
      "31    693\n",
      "32     31\n",
      "33    821\n",
      "34     25\n",
      "35    476\n",
      "36     30\n",
      "37    128\n",
      "      0\n",
      "0   113\n",
      "1   112\n",
      "2   110\n",
      "3   107\n",
      "4   100\n",
      "5    92\n",
      "6    89\n",
      "7    73\n",
      "8    71\n",
      "9    53\n",
      "10   46\n",
      "11   42\n",
      "12   35\n",
      "13   32\n",
      "14   31\n",
      "15   28\n",
      "16   26\n",
      "17   19\n",
      "18    4\n"
     ]
    }
   ],
   "source": [
    "page_cricket=requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "print(page_cricket)\n",
    "soup_cricket=BeautifulSoup(page_cricket.content)\n",
    "soup_cricket\n",
    "# scraping multiple titles:\n",
    "team_names=[]\n",
    "for i in soup_cricket.find_all('span',class_='u-hide-phablet'):\n",
    "    team_names.append(i.text)\n",
    "team_names\n",
    "df_team_names=pd.DataFrame(team_names)\n",
    "print(df_team_names)\n",
    "\n",
    "# scraping multiple matches :\n",
    "\n",
    "matches_with_points=[]\n",
    "for i in soup_cricket.find_all('td',class_='table-body__cell u-center-text'):\n",
    "    matches_with_points.append(i.text)\n",
    "\n",
    "matches_with_points\n",
    "df_matches_with_points=pd.DataFrame(matches_with_points)\n",
    "print(df_matches_with_points)\n",
    "\n",
    "\n",
    "\n",
    "# scraping rating:\n",
    "\n",
    "rating=[]\n",
    "for i in soup_cricket.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    rating.append(i.text)\n",
    "rating\n",
    "df_rating=pd.DataFrame(rating)\n",
    "print(df_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "147e1f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['779',\n",
       " '766',\n",
       " '759',\n",
       " '747',\n",
       " '719',\n",
       " '710',\n",
       " '707',\n",
       " '704',\n",
       " '701',\n",
       " '727',\n",
       " '665',\n",
       " '661',\n",
       " '656',\n",
       " '655',\n",
       " '655',\n",
       " '650',\n",
       " '640',\n",
       " '635',\n",
       " '312',\n",
       " '273',\n",
       " '267',\n",
       " '261',\n",
       " '238',\n",
       " '238',\n",
       " '235',\n",
       " '233',\n",
       " '222',\n",
       " '779',\n",
       " '766',\n",
       " '759',\n",
       " '747',\n",
       " '719',\n",
       " '710',\n",
       " '707',\n",
       " '704',\n",
       " '701',\n",
       " '727',\n",
       " '665',\n",
       " '661',\n",
       " '656',\n",
       " '655',\n",
       " '655',\n",
       " '650',\n",
       " '640',\n",
       " '635',\n",
       " '312',\n",
       " '273',\n",
       " '267',\n",
       " '261',\n",
       " '238',\n",
       " '238',\n",
       " '235',\n",
       " '233',\n",
       " '222']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5.b) Top 10 ODI Batsmen along with the records of their team and rating(SOLVED):\n",
    "\n",
    "page_player=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "print(page_player)\n",
    "soup_batsmen=BeautifulSoup(page_player.content)\n",
    "soup_batsmen\n",
    "\n",
    "batsmen=[]\n",
    "for i in soup_batsmen.find_all('td',class_='table-body__cell name'):\n",
    "    batsmen.append(i.text)\n",
    "batsmen\n",
    "\n",
    "team=[]\n",
    "for j in soup_batsmen.find_all('span',class_='table-body__logo-text'):\n",
    "    team.append(j.text)\n",
    "team\n",
    "\n",
    "ratings=[]\n",
    "for k in soup_batsmen.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    ratings.append(k.text)\n",
    "ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23215f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ---BATSMEN---\n",
      "0            \\nImam-ul-Haq\\n\n",
      "1  \\nRassie van der Dussen\\n\n",
      "2        \\nQuinton de Kock\\n\n",
      "3           \\nDavid Warner\\n\n",
      "4            \\nSteve Smith\\n\n",
      "5         \\nJonny Bairstow\\n\n",
      "6            \\nVirat Kohli\\n\n",
      "7           \\nRohit Sharma\\n\n",
      "8        \\nKane Williamson\\n\n",
      "9         \\nJosh Hazlewood\\n\n",
      "  --TEAM--\n",
      "0      PAK\n",
      "1       SA\n",
      "2       SA\n",
      "3      AUS\n",
      "4      AUS\n",
      "5      ENG\n",
      "6      IND\n",
      "7      IND\n",
      "8       NZ\n",
      "9      AUS\n",
      "  ---RATING---\n",
      "0          779\n",
      "1          766\n",
      "2          759\n",
      "3          747\n",
      "4          719\n",
      "5          710\n",
      "6          707\n",
      "7          704\n",
      "8          701\n",
      "9          727\n",
      "                     BATSMEN TEAM RATINGS\n",
      "0            \\nImam-ul-Haq\\n  PAK     779\n",
      "1  \\nRassie van der Dussen\\n   SA     766\n",
      "2        \\nQuinton de Kock\\n   SA     759\n",
      "3           \\nDavid Warner\\n  AUS     747\n",
      "4            \\nSteve Smith\\n  AUS     719\n",
      "5         \\nJonny Bairstow\\n  ENG     710\n",
      "6            \\nVirat Kohli\\n  IND     707\n",
      "7           \\nRohit Sharma\\n  IND     704\n",
      "8        \\nKane Williamson\\n   NZ     701\n",
      "9         \\nJosh Hazlewood\\n  AUS     727\n"
     ]
    }
   ],
   "source": [
    "# DATAFRAME REPRESENTATION \n",
    "\n",
    "df_batsmen=pd.DataFrame({'---BATSMEN---':batsmen})\n",
    "df_team=pd.DataFrame({'--TEAM--':team})\n",
    "df_ratings=pd.DataFrame({'---RATING---':ratings})\n",
    "print(df_batsmen.head(10))\n",
    "print(df_team.head(10))\n",
    "print(df_ratings.head(10))\n",
    "\n",
    "df_report=pd.DataFrame({'BATSMEN':batsmen,'TEAM':team,'RATINGS':ratings})\n",
    "print(df_report.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7617e8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['779',\n",
       " '766',\n",
       " '759',\n",
       " '747',\n",
       " '719',\n",
       " '710',\n",
       " '707',\n",
       " '704',\n",
       " '701',\n",
       " '727',\n",
       " '665',\n",
       " '661',\n",
       " '656',\n",
       " '655',\n",
       " '655',\n",
       " '650',\n",
       " '640',\n",
       " '635',\n",
       " '312',\n",
       " '273',\n",
       " '267',\n",
       " '261',\n",
       " '238',\n",
       " '238',\n",
       " '235',\n",
       " '233',\n",
       " '222',\n",
       " '779',\n",
       " '766',\n",
       " '759',\n",
       " '747',\n",
       " '719',\n",
       " '710',\n",
       " '707',\n",
       " '704',\n",
       " '701',\n",
       " '727',\n",
       " '665',\n",
       " '661',\n",
       " '656',\n",
       " '655',\n",
       " '655',\n",
       " '650',\n",
       " '640',\n",
       " '635',\n",
       " '312',\n",
       " '273',\n",
       " '267',\n",
       " '261',\n",
       " '238',\n",
       " '238',\n",
       " '235',\n",
       " '233',\n",
       " '222']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q5 C) Top 10 ODI bowlers along with the records of their team and rating(SOLVED).\n",
    "soup_bowler=BeautifulSoup(page_player.content)\n",
    "soup_bowler\n",
    "# scraping bowler name\n",
    "bowler=[]\n",
    "for i in soup_bowler.find_all('td',class_='table-body__cell name'):\n",
    "    bowler.append(i.text)\n",
    "bowler\n",
    "\n",
    "# scraping bowler country\n",
    "country=[]\n",
    "for i in soup_bowler.find_all('span',class_='table-body__logo-text'):\n",
    "    country.append(i.text)\n",
    "country\n",
    "\n",
    "# scraping bowler rating\n",
    "\n",
    "rating_=[]\n",
    "for i in soup_bowler.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    rating_.append(i.text)\n",
    "rating_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "631d83f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0\n",
      "0            \\nImam-ul-Haq\\n\n",
      "1  \\nRassie van der Dussen\\n\n",
      "2        \\nQuinton de Kock\\n\n",
      "3           \\nDavid Warner\\n\n",
      "4            \\nSteve Smith\\n\n",
      "5         \\nJonny Bairstow\\n\n",
      "6            \\nVirat Kohli\\n\n",
      "7           \\nRohit Sharma\\n\n",
      "8        \\nKane Williamson\\n\n",
      "9         \\nJosh Hazlewood\\n\n",
      "---------------------------\n",
      "     0\n",
      "0  PAK\n",
      "1   SA\n",
      "2   SA\n",
      "3  AUS\n",
      "4  AUS\n",
      "5  ENG\n",
      "6  IND\n",
      "7  IND\n",
      "8   NZ\n",
      "9  AUS\n",
      "---------------------------\n",
      "     0\n",
      "0  779\n",
      "1  766\n",
      "2  759\n",
      "3  747\n",
      "4  719\n",
      "5  710\n",
      "6  707\n",
      "7  704\n",
      "8  701\n",
      "9  727\n",
      "---------------------------\n",
      "                        NAME COUNTRY RATING\n",
      "0            \\nImam-ul-Haq\\n     PAK    779\n",
      "1  \\nRassie van der Dussen\\n      SA    766\n",
      "2        \\nQuinton de Kock\\n      SA    759\n",
      "3           \\nDavid Warner\\n     AUS    747\n",
      "4            \\nSteve Smith\\n     AUS    719\n",
      "5         \\nJonny Bairstow\\n     ENG    710\n",
      "6            \\nVirat Kohli\\n     IND    707\n",
      "7           \\nRohit Sharma\\n     IND    704\n",
      "8        \\nKane Williamson\\n      NZ    701\n",
      "9         \\nJosh Hazlewood\\n     AUS    727\n"
     ]
    }
   ],
   "source": [
    "df_bowler=pd.DataFrame(bowler)\n",
    "print(df_bowler.head(10))\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "df_country=pd.DataFrame(country)\n",
    "print(df_country.head(10))\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "df_rating_=pd.DataFrame(rating_)\n",
    "print(df_rating_.head(10))\n",
    "\n",
    "print('---------------------------')\n",
    "\n",
    "df_whole=pd.DataFrame({'NAME':bowler,'COUNTRY':country,'RATING':rating_})\n",
    "\n",
    "print(df_whole.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b1320",
   "metadata": {},
   "source": [
    "# Q 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year ofrelease) and make data frame(SOLVED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "52deda00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n8.5\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.4\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.3\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.2\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.1\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n8.0\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.9\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.8\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.7\\n',\n",
       " '\\n7.6\\n',\n",
       " '\\n7.6\\n',\n",
       " '\\n7.6\\n']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_imdb=requests.get('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "print(page_imdb)\n",
    "soup_imdb=BeautifulSoup(page_imdb.content)\n",
    "soup_imdb\n",
    "\n",
    "# scraping multiple titles of Indian movies \n",
    "\n",
    "indian_movies=[]\n",
    "for i in soup_imdb.find_all('td',class_='titleColumn'):\n",
    "    indian_movies.append(i.text)\n",
    "\n",
    "indian_movies\n",
    "\n",
    "# scraping rating\n",
    "\n",
    "rate=[]\n",
    "for i in soup_imdb.find_all('td',class_='ratingColumn imdbRating'):\n",
    "    rate.append(i.text)\n",
    "    \n",
    "rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4aeaf7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            MOVIE NAME   RATING\n",
      "0    \\n      1.\\n      Ramayana: The Legend of Prin...  \\n8.5\\n\n",
      "1    \\n      2.\\n      Rocketry: The Nambi Effect\\n...  \\n8.4\\n\n",
      "2              \\n      3.\\n      777 Charlie\\n(2022)\\n  \\n8.4\\n\n",
      "3                  \\n      4.\\n      Golmaal\\n(1979)\\n  \\n8.4\\n\n",
      "4                  \\n      5.\\n      Nayakan\\n(1987)\\n  \\n8.4\\n\n",
      "..                                                 ...      ...\n",
      "245           \\n      246.\\n      Goodachari\\n(2018)\\n  \\n7.7\\n\n",
      "246                  \\n      247.\\n      Don\\n(1978)\\n  \\n7.7\\n\n",
      "247                 \\n      248.\\n      Joji\\n(2021)\\n  \\n7.6\\n\n",
      "248              \\n      249.\\n      Aligarh\\n(2015)\\n  \\n7.6\\n\n",
      "249                   \\n      250.\\n      Ko\\n(2011)\\n  \\n7.6\\n\n",
      "\n",
      "[250 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# DF CREATING---\n",
    "df_imdb=pd.DataFrame({'MOVIE NAME':indian_movies,'RATING':rate})\n",
    "print(df_imdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ba7637",
   "metadata": {},
   "source": [
    "# Q 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)and make data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "da6e302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "['\\n1.\\nThe Godfather\\n(1972)\\n', '\\n2.\\nThe Shawshank Redemption\\n(1994)\\n', \"\\n3.\\nSchindler's List\\n(1993)\\n\", '\\n4.\\nRaging Bull\\n(1980)\\n', '\\n5.\\nCasablanca\\n(1942)\\n', '\\n6.\\nCitizen Kane\\n(1941)\\n', '\\n7.\\nGone with the Wind\\n(1939)\\n', '\\n8.\\nThe Wizard of Oz\\n(1939)\\n', \"\\n9.\\nOne Flew Over the Cuckoo's Nest\\n(1975)\\n\", '\\n10.\\nLawrence of Arabia\\n(1962)\\n', '\\n11.\\nVertigo\\n(1958)\\n', '\\n12.\\nPsycho\\n(1960)\\n', '\\n13.\\nThe Godfather Part II\\n(1974)\\n', '\\n14.\\nOn the Waterfront\\n(1954)\\n', '\\n15.\\nSunset Blvd.\\n(1950)\\n', '\\n16.\\nForrest Gump\\n(1994)\\n', '\\n17.\\nThe Sound of Music\\n(1965)\\n', '\\n18.\\n12 Angry Men\\n(1957)\\n', '\\n19.\\nWest Side Story\\n(1961)\\n', '\\n20.\\nStar Wars\\n(1977)\\n', '\\n21.\\n2001: A Space Odyssey\\n(1968)\\n', '\\n22.\\nE.T. the Extra-Terrestrial\\n(1982)\\n', '\\n23.\\nThe Silence of the Lambs\\n(1991)\\n', '\\n24.\\nChinatown\\n(1974)\\n', '\\n25.\\nThe Bridge on the River Kwai\\n(1957)\\n', \"\\n26.\\nSingin' in the Rain\\n(1952)\\n\", \"\\n27.\\nIt's a Wonderful Life\\n(1946)\\n\", '\\n28.\\nDr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb\\n(1964)\\n', '\\n29.\\nSome Like It Hot\\n(1959)\\n', '\\n30.\\nBen-Hur\\n(1959)\\n', '\\n31.\\nApocalypse Now\\n(1979)\\n', '\\n32.\\nAmadeus\\n(1984)\\n', '\\n33.\\nThe Lord of the Rings: The Return of the King\\n(2003)\\n', '\\n34.\\nGladiator\\n(2000)\\n', '\\n35.\\nTitanic\\n(1997)\\n', '\\n36.\\nFrom Here to Eternity\\n(1953)\\n', '\\n37.\\nSaving Private Ryan\\n(1998)\\n', '\\n38.\\nUnforgiven\\n(1992)\\n', '\\n39.\\nRaiders of the Lost Ark\\n(1981)\\n', '\\n40.\\nRocky\\n(1976)\\n', '\\n41.\\nA Streetcar Named Desire\\n(1951)\\n', '\\n42.\\nThe Philadelphia Story\\n(1940)\\n', '\\n43.\\nTo Kill a Mockingbird\\n(1962)\\n', '\\n44.\\nAn American in Paris\\n(1951)\\n', '\\n45.\\nThe Best Years of Our Lives\\n(1946)\\n', '\\n46.\\nMy Fair Lady\\n(1964)\\n', '\\n47.\\nA Clockwork Orange\\n(1971)\\n', '\\n48.\\nDoctor Zhivago\\n(1965)\\n', '\\n49.\\nThe Searchers\\n(1956)\\n', '\\n50.\\nJaws\\n(1975)\\n', '\\n51.\\nPatton\\n(1970)\\n', '\\n52.\\nButch Cassidy and the Sundance Kid\\n(1969)\\n', '\\n53.\\nThe Treasure of the Sierra Madre\\n(1948)\\n', '\\n54.\\nIl buono, il brutto, il cattivo\\n(1966)\\n', '\\n55.\\nThe Apartment\\n(1960)\\n', '\\n56.\\nPlatoon\\n(1986)\\n', '\\n57.\\nHigh Noon\\n(1952)\\n', '\\n58.\\nBraveheart\\n(1995)\\n', '\\n59.\\nDances with Wolves\\n(1990)\\n', '\\n60.\\nJurassic Park\\n(1993)\\n', '\\n61.\\nThe Exorcist\\n(1973)\\n', '\\n62.\\nThe Pianist\\n(2002)\\n', '\\n63.\\nGoodfellas\\n(1990)\\n', '\\n64.\\nThe Deer Hunter\\n(1978)\\n', '\\n65.\\nAll Quiet on the Western Front\\n(1930)\\n', '\\n66.\\nBonnie and Clyde\\n(1967)\\n', '\\n67.\\nThe French Connection\\n(1971)\\n', '\\n68.\\nCity Lights\\n(1931)\\n', '\\n69.\\nIt Happened One Night\\n(1934)\\n', '\\n70.\\nA Place in the Sun\\n(1951)\\n', '\\n71.\\nMidnight Cowboy\\n(1969)\\n', '\\n72.\\nMr. Smith Goes to Washington\\n(1939)\\n', '\\n73.\\nRain Man\\n(1988)\\n', '\\n74.\\nAnnie Hall\\n(1977)\\n', '\\n75.\\nFargo\\n(1996)\\n', '\\n76.\\nGiant\\n(1956)\\n', '\\n77.\\nShane\\n(1953)\\n', '\\n78.\\nThe Grapes of Wrath\\n(1940)\\n', '\\n79.\\nThe Green Mile\\n(1999)\\n', '\\n80.\\nClose Encounters of the Third Kind\\n(1977)\\n', '\\n81.\\nNashville\\n(1975)\\n', '\\n82.\\nNetwork\\n(1976)\\n', '\\n83.\\nThe Graduate\\n(1967)\\n', '\\n84.\\nAmerican Graffiti\\n(1973)\\n', '\\n85.\\nPulp Fiction\\n(1994)\\n', '\\n86.\\nTerms of Endearment\\n(1983)\\n', '\\n87.\\nGood Will Hunting\\n(1997)\\n', '\\n88.\\nThe African Queen\\n(1951)\\n', '\\n89.\\nStagecoach\\n(1939)\\n', '\\n90.\\nMutiny on the Bounty\\n(1935)\\n', '\\n91.\\nThe Great Dictator\\n(1940)\\n', '\\n92.\\nDouble Indemnity\\n(1944)\\n', '\\n93.\\nThe Maltese Falcon\\n(1941)\\n', '\\n94.\\nWuthering Heights\\n(1939)\\n', '\\n95.\\nTaxi Driver\\n(1976)\\n', '\\n96.\\nRear Window\\n(1954)\\n', '\\n97.\\nThe Third Man\\n(1949)\\n', '\\n98.\\nRebel Without a Cause\\n(1955)\\n', '\\n99.\\nNorth by Northwest\\n(1959)\\n', '\\n100.\\nYankee Doodle Dandy\\n(1942)\\n']\n",
      "['\\n\\n\\n\\n\\n\\n\\n\\n9.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n9.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.9\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.7\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.4\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.2\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.5\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.1\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n', '\\n\\n\\n\\n\\n\\n\\n\\n8.3\\n', '\\n\\n\\n\\n\\n\\n\\n\\n7.6\\n']\n"
     ]
    }
   ],
   "source": [
    "page_imdb_all=requests.get('https://www.imdb.com/list/ls055592025/')\n",
    "print(page_imdb_all)\n",
    "soup_imdb_all=BeautifulSoup(page_imdb_all.content)\n",
    "soup_imdb_all\n",
    "\n",
    "#scraping multiple movie name of all time \n",
    "\n",
    "imdb_all=[]\n",
    "for i in soup_imdb_all.find_all('h3',class_='lister-item-header'):\n",
    "    imdb_all.append(i.text)\n",
    "    \n",
    "\n",
    "print(imdb_all)\n",
    "\n",
    "\n",
    "# rating\n",
    "\n",
    "rating_all=[]\n",
    "for i in soup_imdb_all.find_all('div',class_='ipl-rating-star small'):\n",
    "    rating_all.append(i.text)\n",
    "    \n",
    "print(rating_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d095838c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  MOVIE_NAME\n",
      "0              \\n1.\\nThe Godfather\\n(1972)\\n\n",
      "1   \\n2.\\nThe Shawshank Redemption\\n(1994)\\n\n",
      "2           \\n3.\\nSchindler's List\\n(1993)\\n\n",
      "3                \\n4.\\nRaging Bull\\n(1980)\\n\n",
      "4                 \\n5.\\nCasablanca\\n(1942)\\n\n",
      "..                                       ...\n",
      "95              \\n96.\\nRear Window\\n(1954)\\n\n",
      "96            \\n97.\\nThe Third Man\\n(1949)\\n\n",
      "97    \\n98.\\nRebel Without a Cause\\n(1955)\\n\n",
      "98       \\n99.\\nNorth by Northwest\\n(1959)\\n\n",
      "99     \\n100.\\nYankee Doodle Dandy\\n(1942)\\n\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "--------------------------\n",
      "                   RATING\n",
      "0   \\n\\n\\n\\n\\n\\n\\n\\n9.2\\n\n",
      "1   \\n\\n\\n\\n\\n\\n\\n\\n9.3\\n\n",
      "2     \\n\\n\\n\\n\\n\\n\\n\\n9\\n\n",
      "3   \\n\\n\\n\\n\\n\\n\\n\\n8.2\\n\n",
      "4   \\n\\n\\n\\n\\n\\n\\n\\n8.5\\n\n",
      "..                    ...\n",
      "95  \\n\\n\\n\\n\\n\\n\\n\\n8.5\\n\n",
      "96  \\n\\n\\n\\n\\n\\n\\n\\n8.1\\n\n",
      "97  \\n\\n\\n\\n\\n\\n\\n\\n7.6\\n\n",
      "98  \\n\\n\\n\\n\\n\\n\\n\\n8.3\\n\n",
      "99  \\n\\n\\n\\n\\n\\n\\n\\n7.6\\n\n",
      "\n",
      "[100 rows x 1 columns]\n",
      "------------------------\n",
      "                                  MOVIE_NAME                 RATING\n",
      "0              \\n1.\\nThe Godfather\\n(1972)\\n  \\n\\n\\n\\n\\n\\n\\n\\n9.2\\n\n",
      "1   \\n2.\\nThe Shawshank Redemption\\n(1994)\\n  \\n\\n\\n\\n\\n\\n\\n\\n9.3\\n\n",
      "2           \\n3.\\nSchindler's List\\n(1993)\\n    \\n\\n\\n\\n\\n\\n\\n\\n9\\n\n",
      "3                \\n4.\\nRaging Bull\\n(1980)\\n  \\n\\n\\n\\n\\n\\n\\n\\n8.2\\n\n",
      "4                 \\n5.\\nCasablanca\\n(1942)\\n  \\n\\n\\n\\n\\n\\n\\n\\n8.5\\n\n",
      "..                                       ...                    ...\n",
      "95              \\n96.\\nRear Window\\n(1954)\\n  \\n\\n\\n\\n\\n\\n\\n\\n8.5\\n\n",
      "96            \\n97.\\nThe Third Man\\n(1949)\\n  \\n\\n\\n\\n\\n\\n\\n\\n8.1\\n\n",
      "97    \\n98.\\nRebel Without a Cause\\n(1955)\\n  \\n\\n\\n\\n\\n\\n\\n\\n7.6\\n\n",
      "98       \\n99.\\nNorth by Northwest\\n(1959)\\n  \\n\\n\\n\\n\\n\\n\\n\\n8.3\\n\n",
      "99     \\n100.\\nYankee Doodle Dandy\\n(1942)\\n  \\n\\n\\n\\n\\n\\n\\n\\n7.6\\n\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# creating DF\n",
    "\n",
    "\n",
    "df_imdb_all=pd.DataFrame({'MOVIE_NAME':imdb_all})\n",
    "print(df_imdb_all)\n",
    "\n",
    "print('--------------------------')\n",
    "\n",
    "df_rating_all=pd.DataFrame({'RATING':rating_all})\n",
    "print(df_rating_all)\n",
    "\n",
    "print('------------------------')\n",
    "\n",
    "dfx=pd.DataFrame({'MOVIE_NAME':imdb_all,'RATING':rating_all})\n",
    "print(dfx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47ae3d",
   "metadata": {},
   "source": [
    "# Q 9.Write a python program to scrape mentioned details from dineout.co.in \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e066ac8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "['Castle Barbeque', 'Jungle Jamboree', 'Cafe Knosh', 'Castle Barbeque', 'The Barbeque Company', 'India Grill', 'Delhi Barbeque', 'The Monarch - Bar Be Que Village', 'Indian Grill Room']\n",
      "['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium', 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLES</th>\n",
       "      <th>Location</th>\n",
       "      <th>Price</th>\n",
       "      <th>Img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>₹ 2,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>₹ 1,680 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>₹ 3,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>₹ 2,000 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>₹ 1,700 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>₹ 2,400 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>₹ 1,800 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>₹ 1,900 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>₹ 2,200 for 2 (approx)</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             TITLES  \\\n",
       "0                   Castle Barbeque   \n",
       "1                   Jungle Jamboree   \n",
       "2                        Cafe Knosh   \n",
       "3                   Castle Barbeque   \n",
       "4              The Barbeque Company   \n",
       "5                       India Grill   \n",
       "6                    Delhi Barbeque   \n",
       "7  The Monarch - Bar Be Que Village   \n",
       "8                 Indian Grill Room   \n",
       "\n",
       "                                            Location                    Price  \\\n",
       "0                     Connaught Place, Central Delhi  ₹ 2,000 for 2 (approx)    \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi  ₹ 1,680 for 2 (approx)    \n",
       "2  The Leela Ambience Convention Hotel,Shahdara, ...  ₹ 3,000 for 2 (approx)    \n",
       "3             Pacific Mall,Tagore Garden, West Delhi  ₹ 2,000 for 2 (approx)    \n",
       "4                 Gardens Galleria,Sector 38A, Noida  ₹ 1,700 for 2 (approx)    \n",
       "5               Hilton Garden Inn,Saket, South Delhi  ₹ 2,400 for 2 (approx)    \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi  ₹ 1,800 for 2 (approx)    \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad  ₹ 1,900 for 2 (approx)    \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon  ₹ 2,200 for 2 (approx)    \n",
       "\n",
       "                                             Img_url  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "print(page)\n",
    "soup=BeautifulSoup(page.content)\n",
    "soup\n",
    "# scraping all titles\n",
    "titles=[]\n",
    "for i in soup.find_all('a',class_='restnt-name ellipsis'):\n",
    "    titles.append(i.text)\n",
    "print(titles)\n",
    "\n",
    "\n",
    "# scraping all images link \n",
    "images=[]\n",
    "for i in soup.find_all('img',class_='no-img'):\n",
    "    images.append(i.get('data-src'))\n",
    "print(images)\n",
    "\n",
    "\n",
    "#scraping multiple prices\n",
    "price=[]\n",
    "for i in soup.find_all('span',class_='double-line-ellipsis'):\n",
    "    price.append(i.text.split('|')[0])\n",
    "price\n",
    "\n",
    "#scraping location \n",
    "\n",
    "location=[]\n",
    "for i in soup.find_all('div',class_='restnt-loc ellipsis'):\n",
    "    location.append(i.text)\n",
    "location\n",
    "\n",
    "df=pd.DataFrame({'TITLES':titles,'Location':location,'Price':price,'Img_url':images})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ac6081",
   "metadata": {},
   "source": [
    "# Q 8. Write a python program to scrape the details of most downloaded articles from AI in last 90 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5b21f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PAPER TITLES ['Reward is enough', 'Making sense of raw input', 'Law and logic: A review from an argumentation perspective', 'Creativity and artificial intelligence', 'Artificial cognition for social human–robot interaction: An implementation', 'Explanation in artificial intelligence: Insights from the social sciences', 'Making sense of sensory input', 'Conflict-based search for optimal multi-agent pathfinding', 'Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning', 'The Hanabi challenge: A new frontier for AI research', 'Evaluating XAI: A comparison of rule-based and example-based explanations', 'Argumentation in artificial intelligence', 'Algorithms for computing strategies in two-player simultaneous move games', 'Multiple object tracking: A literature review', 'Selection of relevant features and examples in machine learning', 'A survey of inverse reinforcement learning: Challenges, methods and progress', 'Explaining individual predictions when features are dependent: More accurate approximations to Shapley values', 'A review of possible effects of cognitive biases on interpretation of rule-based machine learning models', 'Integrating social power into the decision-making of cognitive agents', \"“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\", 'Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies', 'Algorithm runtime prediction: Methods & evaluation', 'Wrappers for feature subset selection', 'Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics', 'Quantum computation, quantum theory and AI']\n",
      "---------------------------------------------------------------------------------\n",
      "PUBLISHED DATE ['October 2021', 'October 2021', 'October 2015', 'August 1998', 'June 2017', 'February 2019', 'April 2021', 'February 2015', 'August 1999', 'March 2020', 'February 2021', 'October 2007', 'August 2016', 'April 2021', 'December 1997', 'August 2021', 'September 2021', 'June 2021', 'December 2016', 'September 2021', 'May 2021', 'January 2014', 'December 1997', 'October 2021', 'February 2010']\n",
      "----------------------------------------------------------------------------------\n",
      "AUTHORS ['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ', 'Evans, Richard, Bošnjak, Matko and 5 more', 'Prakken, Henry, Sartor, Giovanni ', 'Boden, Margaret A. ', 'Lemaignan, Séverin, Warnier, Mathieu and 3 more', 'Miller, Tim ', 'Evans, Richard, Hernández-Orallo, José and 3 more', 'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ', 'Sutton, Richard S., Precup, Doina, Singh, Satinder ', 'Bard, Nolan, Foerster, Jakob N. and 13 more', 'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ', 'Bench-Capon, T.J.M., Dunne, Paul E. ', 'Bošanský, Branislav, Lisý, Viliam and 3 more', 'Luo, Wenhan, Xing, Junliang and 4 more', 'Blum, Avrim L., Langley, Pat ', 'Arora, Saurabh, Doshi, Prashant ', 'Aas, Kjersti, Jullum, Martin, Løland, Anders ', 'Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes ', 'Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. ', 'Riveiro, Maria, Thill, Serge ', 'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ', 'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ', 'Kohavi, Ron, John, George H. ', 'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ', 'Ying, Mingsheng ']\n",
      "-------------------------------------------------------------------------------------\n",
      "PAPER_URL ['https://www.sciencedirect.com/science/article/pii/S0004370221000862', 'https://www.sciencedirect.com/science/article/pii/S0004370221000722', 'https://www.sciencedirect.com/science/article/pii/S0004370215000910', 'https://www.sciencedirect.com/science/article/pii/S0004370298000551', 'https://www.sciencedirect.com/science/article/pii/S0004370216300790', 'https://www.sciencedirect.com/science/article/pii/S0004370218305988', 'https://www.sciencedirect.com/science/article/pii/S0004370220301855', 'https://www.sciencedirect.com/science/article/pii/S0004370214001386', 'https://www.sciencedirect.com/science/article/pii/S0004370299000521', 'https://www.sciencedirect.com/science/article/pii/S0004370219300116', 'https://www.sciencedirect.com/science/article/pii/S0004370220301533', 'https://www.sciencedirect.com/science/article/pii/S0004370207000793', 'https://www.sciencedirect.com/science/article/pii/S0004370216300285', 'https://www.sciencedirect.com/science/article/pii/S0004370220301958', 'https://www.sciencedirect.com/science/article/pii/S0004370297000635', 'https://www.sciencedirect.com/science/article/pii/S0004370221000515', 'https://www.sciencedirect.com/science/article/pii/S0004370221000539', 'https://www.sciencedirect.com/science/article/pii/S0004370221000096', 'https://www.sciencedirect.com/science/article/pii/S0004370216300868', 'https://www.sciencedirect.com/science/article/pii/S0004370221000588', 'https://www.sciencedirect.com/science/article/pii/S0004370221000102', 'https://www.sciencedirect.com/science/article/pii/S0004370213001082', 'https://www.sciencedirect.com/science/article/pii/S000437029700043X', 'https://www.sciencedirect.com/science/article/pii/S0004370221000734', 'https://www.sciencedirect.com/science/article/pii/S0004370209001398']\n",
      "**************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "page_AI=requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles') # REQUESTING FOR WEB ACCESS\n",
    "page_AI\n",
    "soup_AI=BeautifulSoup(page_AI.content) # CREATING INSTANCE OF BEAUTIFULSOUP CLASS\n",
    "soup_AI\n",
    "\n",
    "#SCRAPING MULTIPLE TITLES\n",
    "\n",
    "paper_title=[]\n",
    "for i in soup_AI.find_all('h2',class_='sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg'):\n",
    "    paper_title.append(i.text)\n",
    "    \n",
    "print('PAPER TITLES',paper_title)\n",
    "\n",
    "print('---------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "\n",
    "# SCRAPING PUBLISHED DATE\n",
    "\n",
    "published_date=[]\n",
    "for i in soup_AI.find_all('span',class_='sc-1thf9ly-2 dvggWt'):\n",
    "    published_date.append(i.text)\n",
    "    \n",
    "print('PUBLISHED DATE',published_date)\n",
    "print('----------------------------------------------------------------------------------')\n",
    "\n",
    "# SCRAPING AUTHOR\n",
    "\n",
    "author=[]\n",
    "for i in soup_AI.find_all('span',class_='sc-1w3fpd7-0 dnCnAO'):\n",
    "    author.append(i.text)\n",
    "    \n",
    "print('AUTHORS',author)\n",
    "\n",
    "print('-------------------------------------------------------------------------------------')\n",
    "    \n",
    "# SCRAPING PAPER URL\n",
    "\n",
    "paper_url=[]\n",
    "for i in soup_AI.find_all('a',class_='sc-5smygv-0 fIXTHm'):\n",
    "    paper_url.append(i.get('href'))\n",
    "    \n",
    "print('PAPER_URL',paper_url)\n",
    "\n",
    "print('**************************************************************************************')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5bbadca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          PAPER TITLE  PUBLISHED DATE  \\\n",
      "0                                    Reward is enough    October 2021   \n",
      "1                           Making sense of raw input    October 2021   \n",
      "2   Law and logic: A review from an argumentation ...    October 2015   \n",
      "3              Creativity and artificial intelligence     August 1998   \n",
      "4   Artificial cognition for social human–robot in...       June 2017   \n",
      "5   Explanation in artificial intelligence: Insigh...   February 2019   \n",
      "6                       Making sense of sensory input      April 2021   \n",
      "7   Conflict-based search for optimal multi-agent ...   February 2015   \n",
      "8   Between MDPs and semi-MDPs: A framework for te...     August 1999   \n",
      "9   The Hanabi challenge: A new frontier for AI re...      March 2020   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   February 2021   \n",
      "11           Argumentation in artificial intelligence    October 2007   \n",
      "12  Algorithms for computing strategies in two-pla...     August 2016   \n",
      "13      Multiple object tracking: A literature review      April 2021   \n",
      "14  Selection of relevant features and examples in...   December 1997   \n",
      "15  A survey of inverse reinforcement learning: Ch...     August 2021   \n",
      "16  Explaining individual predictions when feature...  September 2021   \n",
      "17  A review of possible effects of cognitive bias...       June 2021   \n",
      "18  Integrating social power into the decision-mak...   December 2016   \n",
      "19  “That's (not) the output I expected!” On the r...  September 2021   \n",
      "20  Explaining black-box classifiers using post-ho...        May 2021   \n",
      "21  Algorithm runtime prediction: Methods & evalua...    January 2014   \n",
      "22              Wrappers for feature subset selection   December 1997   \n",
      "23  Commonsense visual sensemaking for autonomous ...    October 2021   \n",
      "24         Quantum computation, quantum theory and AI   February 2010   \n",
      "\n",
      "                                              AUTHORS  \\\n",
      "0   Silver, David, Singh, Satinder, Precup, Doina,...   \n",
      "1           Evans, Richard, Bošnjak, Matko and 5 more   \n",
      "2                   Prakken, Henry, Sartor, Giovanni    \n",
      "3                                 Boden, Margaret A.    \n",
      "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more   \n",
      "5                                        Miller, Tim    \n",
      "6   Evans, Richard, Hernández-Orallo, José and 3 more   \n",
      "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   \n",
      "8   Sutton, Richard S., Precup, Doina, Singh, Sati...   \n",
      "9         Bard, Nolan, Foerster, Jakob N. and 13 more   \n",
      "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   \n",
      "11               Bench-Capon, T.J.M., Dunne, Paul E.    \n",
      "12       Bošanský, Branislav, Lisý, Viliam and 3 more   \n",
      "13             Luo, Wenhan, Xing, Junliang and 4 more   \n",
      "14                      Blum, Avrim L., Langley, Pat    \n",
      "15                   Arora, Saurabh, Doshi, Prashant    \n",
      "16      Aas, Kjersti, Jullum, Martin, Løland, Anders    \n",
      "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...   \n",
      "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    \n",
      "19                      Riveiro, Maria, Thill, Serge    \n",
      "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...   \n",
      "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...   \n",
      "22                      Kohavi, Ron, John, George H.    \n",
      "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...   \n",
      "24                                   Ying, Mingsheng    \n",
      "\n",
      "                                        URLS OF PAPER  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n",
      "24  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "# CREATING DF----\n",
    "df_AI=pd.DataFrame({'PAPER TITLE':paper_title,'PUBLISHED DATE':published_date,'AUTHORS':author,'URLS OF PAPER':paper_url})\n",
    "print(df_AI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49c9bcd",
   "metadata": {},
   "source": [
    "# Q 10 .Write a python program to scrape the details of top publications from Google Scholar from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6b2e637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "['Nature', 'The New England Journal of Medicine', 'Science', 'IEEE/CVF Conference on Computer Vision and Pattern Recognition', 'The Lancet', 'Advanced Materials', 'Nature Communications', 'Cell', 'International Conference on Learning Representations', 'Neural Information Processing Systems', 'JAMA', 'Chemical Reviews', 'Proceedings of the National Academy of Sciences', 'Angewandte Chemie', 'Chemical Society Reviews', 'Journal of the American Chemical Society', 'IEEE/CVF International Conference on Computer Vision', 'Nucleic Acids Research', 'International Conference on Machine Learning', 'Nature Medicine', 'Renewable and Sustainable Energy Reviews', 'Science of The Total Environment', 'Advanced Energy Materials', 'Journal of Clinical Oncology', 'ACS Nano', 'Journal of Cleaner Production', 'Advanced Functional Materials', 'Physical Review Letters', 'Scientific Reports', 'The Lancet Oncology', 'Energy & Environmental Science', 'IEEE Access', 'PLoS ONE', 'Science Advances', 'Journal of the American College of Cardiology', 'Applied Catalysis B: Environmental', 'Nature Genetics', 'BMJ', 'Circulation', 'European Conference on Computer Vision', 'International Journal of Molecular Sciences', 'Nature Materials', 'Chemical engineering journal', 'AAAI Conference on Artificial Intelligence', 'Journal of Materials Chemistry A', 'ACS Applied Materials & Interfaces', 'Nature Biotechnology', 'The Lancet Infectious Diseases', 'Frontiers in Immunology', 'Applied Energy', 'Nano Energy', 'Nature Energy', 'Meeting of the Association for Computational Linguistics (ACL)', 'The Astrophysical Journal', 'Gastroenterology', 'Nature Methods', 'IEEE Transactions on Pattern Analysis and Machine Intelligence', 'Cochrane Database of Systematic Reviews', 'Blood', 'Neuron', 'Nano Letters', 'Morbidity and Mortality Weekly Report', 'European Heart Journal', 'Nature Nanotechnology', 'ACS Catalysis', 'Nature Neuroscience', 'American Economic Review', 'Journal of High Energy Physics', 'IEEE Communications Surveys & Tutorials', 'Annals of Oncology', 'Nutrients', 'Accounts of Chemical Research', 'Immunity', 'Environmental Science & Technology', 'Nature Reviews. Molecular Cell Biology', 'Gut', 'Physical Review D', 'ACS Energy Letters', 'Monthly Notices of the Royal Astronomical Society', 'Conference on Empirical Methods in Natural Language Processing (EMNLP)', 'Clinical Infectious Diseases', 'Cell Metabolism', 'Nature Reviews Immunology', 'Joule', 'Nature Photonics', 'International Journal of Environmental Research and Public Health', 'Environmental Pollution', 'Computers in Human Behavior', 'Frontiers in Microbiology', 'Nature Physics', 'Small', 'Cell Reports', 'Molecular Cell', 'Clinical Cancer Research', 'Bioresource Technology', 'Journal of Business Research', 'Molecular Cancer', 'Sensors', 'Nature Climate Change', 'IEEE Internet of Things Journal']\n",
      "******************************************************************************\n",
      "['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '17.', '18.', '19.', '20.', '21.', '22.', '23.', '24.', '25.', '26.', '27.', '28.', '29.', '30.', '31.', '32.', '33.', '34.', '35.', '36.', '37.', '38.', '39.', '40.', '41.', '42.', '43.', '44.', '45.', '46.', '47.', '48.', '49.', '50.', '51.', '52.', '53.', '54.', '55.', '56.', '57.', '58.', '59.', '60.', '61.', '62.', '63.', '64.', '65.', '66.', '67.', '68.', '69.', '70.', '71.', '72.', '73.', '74.', '75.', '76.', '77.', '78.', '79.', '80.', '81.', '82.', '83.', '84.', '85.', '86.', '87.', '88.', '89.', '90.', '91.', '92.', '93.', '94.', '95.', '96.', '97.', '98.', '99.', '100.']\n",
      "**********************************************************************************\n",
      "['444', '432', '401', '389', '354', '312', '307', '300', '286', '278', '267', '265', '256', '245', '244', '242', '239', '238', '237', '235', '227', '225', '220', '213', '211', '211', '210', '207', '206', '202', '202', '200', '198', '197', '195', '192', '191', '190', '189', '186', '183', '181', '181', '180', '178', '177', '175', '173', '173', '173', '172', '170', '169', '167', '166', '165', '165', '165', '165', '164', '164', '163', '163', '163', '163', '162', '160', '160', '159', '159', '159', '159', '158', '158', '155', '155', '155', '155', '155', '154', '153', '153', '152', '152', '152', '152', '152', '152', '151', '151', '150', '149', '149', '146', '146', '145', '145', '145', '144', '144']\n",
      "-------------------------------------------------------------\n",
      "['667', '780', '614', '627', '635', '418', '428', '505', '533', '436', '425', '444', '364', '332', '386', '344', '415', '550', '421', '389', '324', '311', '300', '315', '277', '273', '280', '294', '274', '329', '290', '303', '278', '294', '276', '246', '297', '307', '301', '321', '253', '265', '224', '296', '220', '223', '315', '296', '228', '217', '232', '314', '304', '234', '254', '296', '293', '243', '229', '231', '207', '302', '265', '264', '220', '248', '263', '220', '304', '243', '214', '211', '242', '214', '340', '235', '217', '212', '194', '249', '278', '211', '292', '233', '228', '225', '222', '214', '225', '222', '196', '205', '202', '201', '190', '233', '209', '201', '228', '212']\n",
      "--------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "page_gs=requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "print(page_gs)\n",
    "\n",
    "soup_gs=BeautifulSoup(page_gs.content)\n",
    "soup_gs\n",
    "\n",
    "# SCRAPING PUBLICATION \n",
    "\n",
    "publication=[]\n",
    "for i in soup_gs.find_all('td',class_='gsc_mvt_t'):\n",
    "    publication.append(i.text)\n",
    "    \n",
    "print(publication)\n",
    "print('******************************************************************************')\n",
    "\n",
    "# scraping rank\n",
    "\n",
    "rank=[]\n",
    "for i in soup_gs.find_all('td',class_='gsc_mvt_p'):\n",
    "    rank.append(i.text)\n",
    "    \n",
    "print(rank)\n",
    "\n",
    "print('**********************************************************************************')\n",
    "\n",
    "\n",
    "#scraping h5 index\n",
    "\n",
    "h5_index=[]\n",
    "for i in soup_gs.find_all('a',class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5_index.append(i.text)\n",
    "    \n",
    "print(h5_index)\n",
    "\n",
    "print('-------------------------------------------------------------')\n",
    "\n",
    "\n",
    "# scraping h5 median\n",
    "\n",
    "h5_median=[]\n",
    "for i in soup_gs.find_all('span',class_='gs_ibl gsc_mp_anchor'):\n",
    "    h5_median.append(i.text)\n",
    "    \n",
    "print(h5_median)\n",
    "\n",
    "print('--------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "37377ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>PUBLICATION</th>\n",
       "      <th>h5 index</th>\n",
       "      <th>h5_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    RANK                                        PUBLICATION h5 index h5_median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# CREATING DATAFRAME OF ABOVE DATA\n",
    "df_gs=pd.DataFrame({'RANK':rank,'PUBLICATION':publication,'h5 index':h5_index,'h5_median':h5_median})\n",
    "df_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acce454",
   "metadata": {},
   "source": [
    "# Q6.Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape: a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating. b) Top 10 women’s ODI Batting players along with the records of their team and rating. c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "8509724d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "                        ---WOMEN CRICKET TEAM---\n",
      "0  \\n2\\n\\n\\nSouth Africa\\nSA\\n\\n26\\n3,098\\n119\\n\n",
      "1      \\n3\\n\\n\\nEngland\\nENG\\n\\n25\\n2,904\\n116\\n\n",
      "2        \\n4\\n\\n\\nIndia\\nIND\\n\\n27\\n2,820\\n104\\n\n",
      "3   \\n5\\n\\n\\nNew Zealand\\nNZ\\n\\n24\\n2,425\\n101\\n\n",
      "4    \\n6\\n\\n\\nWest Indies\\nWI\\n\\n24\\n2,334\\n97\\n\n",
      "5      \\n7\\n\\n\\nBangladesh\\nBAN\\n\\n12\\n932\\n78\\n\n",
      "6         \\n8\\n\\n\\nThailand\\nTHA\\n\\n8\\n572\\n72\\n\n",
      "7      \\n9\\n\\n\\nPakistan\\nPAK\\n\\n24\\n1,519\\n63\\n\n",
      "8        \\n10\\n\\n\\nSri Lanka\\nSL\\n\\n8\\n353\\n44\\n\n",
      "9        \\n11\\n\\n\\nIreland\\nIRE\\n\\n14\\n548\\n39\\n\n",
      "-----------------------------------------------------------------------------------------\n",
      "<Response [200]>\n",
      "       ---WOMEN BATTER----\n",
      "0          \\nBeth Mooney\\n\n",
      "1      \\nLaura Wolvaardt\\n\n",
      "2       \\nNatalie Sciver\\n\n",
      "3     \\nHarmanpreet Kaur\\n\n",
      "4      \\nSmriti Mandhana\\n\n",
      "5          \\nMeg Lanning\\n\n",
      "6       \\nRachael Haynes\\n\n",
      "7    \\nAmy Satterthwaite\\n\n",
      "8  \\nChamari Athapaththu\\n\n",
      "9        \\nJess Jonassen\\n\n",
      "  ----WOMEN COUNTRY NAME----\n",
      "0                        AUS\n",
      "1                         SA\n",
      "2                        ENG\n",
      "3                        IND\n",
      "4                        IND\n",
      "5                        AUS\n",
      "6                        AUS\n",
      "7                         NZ\n",
      "8                         SL\n",
      "9                        AUS\n",
      "  -----RATING------\n",
      "0               749\n",
      "1               732\n",
      "2               725\n",
      "3               716\n",
      "4               714\n",
      "5               710\n",
      "6               701\n",
      "7               661\n",
      "8               655\n",
      "9               725\n",
      "-----------------------------------------------------------------------------------\n",
      "-------------------------------------------------WOMEN BATTER LIST-----------------------------------------------------\n",
      "              WOMEN BATTER COUNTRY RATING\n",
      "0          \\nBeth Mooney\\n     AUS    749\n",
      "1      \\nLaura Wolvaardt\\n      SA    732\n",
      "2       \\nNatalie Sciver\\n     ENG    725\n",
      "3     \\nHarmanpreet Kaur\\n     IND    716\n",
      "4      \\nSmriti Mandhana\\n     IND    714\n",
      "5          \\nMeg Lanning\\n     AUS    710\n",
      "6       \\nRachael Haynes\\n     AUS    701\n",
      "7    \\nAmy Satterthwaite\\n      NZ    661\n",
      "8  \\nChamari Athapaththu\\n      SL    655\n",
      "9        \\nJess Jonassen\\n     AUS    725\n",
      "-----------------------------------------------------------------------------------\n",
      "<Response [200]>\n",
      "---------------------------ALL ROUNDER WOMEN PLAYER RATING------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PLAYER</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>RATING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nEllyse Perry\\n</td>\n",
       "      <td>AUS</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nNatalie Sciver\\n</td>\n",
       "      <td>ENG</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nAmelia Kerr\\n</td>\n",
       "      <td>NZ</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nMarizanne Kapp\\n</td>\n",
       "      <td>SA</td>\n",
       "      <td>349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nDeepti Sharma\\n</td>\n",
       "      <td>IND</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\nAshleigh Gardner\\n</td>\n",
       "      <td>AUS</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\nJess Jonassen\\n</td>\n",
       "      <td>AUS</td>\n",
       "      <td>246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nJhulan Goswami\\n</td>\n",
       "      <td>IND</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\nKatherine Brunt\\n</td>\n",
       "      <td>ENG</td>\n",
       "      <td>207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\nNida Dar\\n</td>\n",
       "      <td>PAK</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 PLAYER TEAM RATING\n",
       "0      \\nEllyse Perry\\n  AUS    374\n",
       "1    \\nNatalie Sciver\\n  ENG    357\n",
       "2       \\nAmelia Kerr\\n   NZ    356\n",
       "3    \\nMarizanne Kapp\\n   SA    349\n",
       "4     \\nDeepti Sharma\\n  IND    322\n",
       "5  \\nAshleigh Gardner\\n  AUS    270\n",
       "6     \\nJess Jonassen\\n  AUS    246\n",
       "7    \\nJhulan Goswami\\n  IND    214\n",
       "8   \\nKatherine Brunt\\n  ENG    207\n",
       "9          \\nNida Dar\\n  PAK    205"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SOLUTION OF a)\n",
    "\n",
    "# requesting web access\n",
    "page_women=requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "print(page_women)\n",
    "\n",
    "#creating object of class\n",
    "\n",
    "soup_women=BeautifulSoup(page_women.content)\n",
    "soup_women\n",
    "\n",
    "#scraping \n",
    "\n",
    "women_team=[]\n",
    "for i in soup_women.find_all('tr',class_='table-body'):\n",
    "    women_team.append(i.text)\n",
    "    \n",
    "df=pd.DataFrame({'---WOMEN CRICKET TEAM---':women_team})\n",
    "print(df.head(10))\n",
    "\n",
    "print('-----------------------------------------------------------------------------------------')\n",
    "\n",
    "# SOLUTION TO b)\n",
    "\n",
    "# new page request for women ranking \n",
    "page_ranking=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "print(page_ranking) # request\n",
    "\n",
    "# new instance creation \n",
    "soup_ranking=BeautifulSoup(page_ranking.content)\n",
    "soup_ranking\n",
    " \n",
    "    # women player name scraping \n",
    "women_batter=[]\n",
    "for i in soup_ranking.find_all('td',class_='table-body__cell name'):\n",
    "    women_batter.append(i.text)\n",
    "\n",
    "             \n",
    "df_women_batter=pd.DataFrame({'---WOMEN BATTER----':women_batter})\n",
    "print(df_women_batter.head(10))\n",
    "\n",
    " # women COUNTRY name scraping \n",
    "\n",
    "women_country=[]\n",
    "for i in soup_ranking.find_all('span',class_='table-body__logo-text'):\n",
    "    women_country.append(i.text)\n",
    "    \n",
    "df_wc=pd.DataFrame({'----WOMEN COUNTRY NAME----':women_country})\n",
    "print(df_wc.head(10))\n",
    "\n",
    " # women rating  scraping \n",
    "\n",
    "women_rat=[]\n",
    "for i in soup_ranking.find_all('td',class_='table-body__cell u-text-right rating'):\n",
    "    women_rat.append(i.text)\n",
    "    \n",
    "df_wr=pd.DataFrame({'-----RATING------':women_rat})\n",
    "print(df_wr.head(10))\n",
    "\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "print('-------------------------------------------------WOMEN BATTER LIST-----------------------------------------------------')\n",
    "df_final=pd.DataFrame({'WOMEN BATTER':women_batter,'COUNTRY':women_country,'RATING':women_rat})\n",
    "print(df_final.head(10))\n",
    "\n",
    "print('-----------------------------------------------------------------------------------')\n",
    "\n",
    "#SOLUTION TO c)\n",
    "\n",
    "page_ar=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder')\n",
    "print(page_ar)\n",
    "soup_ar=BeautifulSoup(page_ar.content)\n",
    "soup_ar\n",
    "\n",
    " # scraping player name\n",
    "    \n",
    "ar_player=[]\n",
    "for i in soup_ar.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    ar_player.append(i.text)\n",
    "\n",
    "\n",
    "\n",
    " # scraping TEAM\n",
    "    \n",
    "team=[]\n",
    "for i in soup_ar.find_all('span',class_='table-body__logo-text'):\n",
    "    team.append(i.text)\n",
    " \n",
    "# scraping rating\n",
    "\n",
    "rating=[]\n",
    "for i in soup_ar.find_all('td',class_='table-body__cell rating'):\n",
    "    rating.append(i.text)\n",
    "    \n",
    "print('---------------------------ALL ROUNDER WOMEN PLAYER RATING------------------------------')\n",
    "df_AR_full=pd.DataFrame({\"PLAYER\":ar_player,'TEAM':team,'RATING':rating})\n",
    "df_AR_full.head(10)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0d349",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8dbc0470",
   "metadata": {},
   "source": [
    "# Q.1. # LIST OF ALL HEADER TAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e1fd6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfl-h2\"><span id=\"From_today.27s_featured_list\"></span><span class=\"mw-headline\" id=\"From_today's_featured_list\">From today's featured list</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs_header = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs_header.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a09bf5",
   "metadata": {},
   "source": [
    "# Q.7. Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :i) Headline ii) Time iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "59239ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<h1 class=\"ArticleHeader-headline\">Tesla CEO Elon Musk kicks off first Semi truck deliveries</h1>\n",
      "[]\n",
      "['https://www.cnbc.com/technology/']\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import time\n",
    "\n",
    "# NEWS- Tesla CEO Elon Musk kicks off first Semi truck deliveries\n",
    "page_news=requests.get('https://www.cnbc.com/2022/12/01/tesla-ceo-elon-musk-kicks-off-semi-truck-deliveries.html')\n",
    "print(page_news)\n",
    "\n",
    "soup_news=BeautifulSoup(page_news.content)\n",
    "soup_news\n",
    "\n",
    "              # scraping headline\n",
    "headline=soup_news.find('h1',class_='ArticleHeader-headline')\n",
    "print(headline)\n",
    "              # time\n",
    "time=[i.text for i in soup_news.find_all(\"div\",class_=\"ArticleHeader-time\") ] # Scrap Time\n",
    "print(time)\n",
    "\n",
    "link=[]\n",
    "for i in soup_news.find_all('a',class_='ArticleHeader-eyebrow'):\n",
    "    link.append(i.get('href'))\n",
    "\n",
    "print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde2a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
